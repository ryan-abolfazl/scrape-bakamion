from selenium.webdriver import Firefox
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import re
import time
import random
import csv
import os
import pandas as pd
import openpyxl
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
xpath_click_target = '//*[@id="get_seller_phone"]'
batch_size = 1
delay_between = (1, 3)
max_consecutive_errors = 5


def get_latest_post_id():
    print("ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ú¯Ø±ÙØªÙ† Ø¢Ø®Ø±ÛŒÙ† Ø¢Ú¯Ù‡ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ...")
    driver = Firefox()
    driver.get("https://bakamion.ir/ads")
    time.sleep(3)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    link_tag = soup.find("a", href=re.compile(r"/ad/\d+"))
    if not link_tag:
        print("âŒ Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú© Ø¢Ú¯Ù‡ÛŒâ€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
        return None

    href = link_tag["href"]
    post_id = int(href.split("/ad/")[1])
    print(f"âœ… Ø¢Ø®Ø±ÛŒÙ† Ø¢Ú¯Ù‡ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯: ID = {post_id}")
    return post_id


def crawl_ad(post_id, csv_writer):
    url = f"https://bakamion.ir/ad/{post_id}"
    print(f"ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ú¯Ù‡ÛŒ: {url}")
    try:
        driver = Firefox()
        driver.get(url)
        time.sleep(3)

        try:
            btn = driver.find_element(By.XPATH, xpath_click_target)
            btn.click()
            time.sleep(1)
        except Exception as e:
            print(f"âš ï¸ Ø¯Ú©Ù…Ù‡ Ú©Ù„ÛŒÚ© Ù†Ø´Ø¯: {e}")

        soup = BeautifulSoup(driver.page_source, "html.parser")
        driver.quit()

        name_tag = soup.find("span", class_="fs-6 fs-lg-5 fw-bold text-dark")
        name = name_tag.text.strip() if name_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"

        labels = soup.find_all("div", class_="col-4 col-lg-3 fs-7 fs-lg-6 text-body-secondary fw-normal")
        values = soup.find_all("div", class_="d-flex justify-content-end justify-content-lg-start align-items-center col-8 col-lg-9 fs-7 fs-lg-6 text-body fw-bold")

        info_dict = {}
        for label_tag, value_tag in zip(labels, values):
            label = label_tag.text.strip()
            value = value_tag.text.strip()
            info_dict[label] = value

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
        city = info_dict.get("Ø§Ø³ØªØ§Ù†", "")
        year = info_dict.get("Ø³Ø§Ù„", "")
        raw_phone = info_dict.get("Ù‚ÛŒÙ…Øª", "")
        match = re.search(r'\d{10}', raw_phone)
        phone = match.group() if match else ""

        # Ù†ÙˆØ´ØªÙ† Ø¯Ø± ÙØ§ÛŒÙ„ CSV
        csv_writer.writerow([name, city, year, phone])

        print("âœ…", name)
        print(info_dict)
        print("-" * 40)
        return True

    except Exception as e:
        print(f"â— Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ú¯Ù‡ÛŒ {post_id}: {e}")
        return False


def crawl_batch():
    latest_id = get_latest_post_id()
    if latest_id is None:
        print("â›” Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
        return

    # ğŸ”¸ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„ CSV
    file_exists = os.path.exists("output.csv")
    csv_file = open("output.csv", "a", newline="", encoding="utf-8")
    csv_writer = csv.writer(csv_file)
    if not file_exists:
        csv_writer.writerow(["Ù†Ø§Ù…", "Ø´Ù‡Ø±", "Ø³Ø§Ù„ Ø³Ø§Ø®Øª", "Ø´Ù…Ø§Ø±Ù‡ ØªÙ…Ø§Ø³"])

    not_found = 0

    for post_id in range(latest_id, latest_id - batch_size, -1):
        success = crawl_ad(post_id, csv_writer)
        if not success:
            not_found += 1
            if not_found >= max_consecutive_errors:
                print("â¸ ØªÙˆÙ‚Ù Û¶Û° Ø«Ø§Ù†ÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù¾ÛŒØ§Ù¾ÛŒ.")
                time.sleep(60)
                not_found = 0
        else:
            not_found = 0

        time.sleep(random.uniform(*delay_between))

    csv_file.close()
    print("ğŸ“ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ output.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    df = pd.read_csv("output.csv", encoding="utf-8")
    df.to_excel("output.xlsx", index=False)
    print("ğŸ“ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ output.xlsx Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")


# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
crawl_batch()