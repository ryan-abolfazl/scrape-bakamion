from selenium.webdriver import Firefox
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import re
import time
import random
import csv
import os
import pandas as pd
import openpyxl


# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
xpath_click_target = '//*[@id="get_seller_phone"]'
# batch_size = 1
delay_between = (1, 3)
max_consecutive_errors = 5


def extract_ad_links():
    print("ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¢Ú¯Ù‡ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ...")
    driver = Firefox()
    driver.get("https://bakamion.ir/ads")
    time.sleep(3)

    # ÙÙ‚Ø· ÛŒÚ©â€ŒØ¨Ø§Ø± Ø§Ø³Ú©Ø±ÙˆÙ„ ØªØ§ Ù¾Ø§ÛŒÛŒÙ†
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(25)

    # Ú¯Ø±ÙØªÙ† HTML Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø³Ú©Ø±ÙˆÙ„
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
    ad_links = set()
    for tag in soup.find_all("a", href=True):
        href = tag["href"]
        # print(href)
        if re.match("https://bakamion.ir/ad/", href):
            ad_links.add(href)

    sorted_links = sorted(ad_links, reverse=True)
    links = [link for link in sorted_links if "create" not in link]
    print(f" Ø¢Ú¯Ù‡ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯{len(links)}")
    return links


def crawl_ad(link, csv_writer):
    url = link
    print(f"ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ú¯Ù‡ÛŒ: {url}")
    try:
        driver = Firefox()
        driver.get(url)
        time.sleep(3)

        try:
            btn = driver.find_element(By.XPATH, xpath_click_target)
            btn.click()
            time.sleep(1)
        except Exception as e:
            print(f"âš ï¸ Ø¯Ú©Ù…Ù‡ Ú©Ù„ÛŒÚ© Ù†Ø´Ø¯: {e}")

        soup = BeautifulSoup(driver.page_source, "html.parser")
        driver.quit()

        name_tag = soup.find("span", class_="fs-6 fs-lg-5 fw-bold text-dark")
        name = name_tag.text.strip() if name_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"

        labels = soup.find_all("div", class_="col-4 col-lg-3 fs-7 fs-lg-6 text-body-secondary fw-normal")
        values = soup.find_all("div", class_="d-flex justify-content-end justify-content-lg-start align-items-center col-8 col-lg-9 fs-7 fs-lg-6 text-body fw-bold")

        info_dict = {}
        for label_tag, value_tag in zip(labels, values):
            label = label_tag.text.strip()
            value = value_tag.text.strip()
            info_dict[label] = value

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
        city = info_dict.get("Ø§Ø³ØªØ§Ù†", "")
        year = info_dict.get("Ø³Ø§Ù„", "")
        raw_phone = info_dict.get("Ù‚ÛŒÙ…Øª", "")
        match = re.search(r'\d{10}', raw_phone)
        phone = match.group() if match else ""

        # Ù†ÙˆØ´ØªÙ† Ø¯Ø± ÙØ§ÛŒÙ„ CSV
        csv_writer.writerow([name, city, year, phone])

        print("âœ…", name)
        print(info_dict)
        print("-" * 40)
        return True

    except Exception as e:
        print(f"â— Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ú¯Ù‡ÛŒ {url}: {e}")
        return False

def crawl_batch():
    urls = extract_ad_links()
    if urls is None:
        print("â›” Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
        return

    # ğŸ”¸ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„ CSV
    file_exists = os.path.exists("output1.csv")
    csv_file = open("output1.csv", "a", newline="", encoding="utf-8")
    csv_writer = csv.writer(csv_file)
    if not file_exists:
        csv_writer.writerow(["Ù†Ø§Ù…", "Ø´Ù‡Ø±", "Ø³Ø§Ù„ Ø³Ø§Ø®Øª", "Ø´Ù…Ø§Ø±Ù‡ ØªÙ…Ø§Ø³"])

    not_found = 0
    found = 0
    # max_founds = 2
    for link in urls:
        success = crawl_ad(link, csv_writer)
        if not success:
            not_found += 1
            if not_found >= max_consecutive_errors:
                print("â¸ ØªÙˆÙ‚Ù Û¶Û° Ø«Ø§Ù†ÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù¾ÛŒØ§Ù¾ÛŒ.")
                time.sleep(60)
                not_found = 0
        else:
            not_found = 0
            found += 1
            if found == 3:
                break


        time.sleep(random.uniform(*delay_between))

    csv_file.close()
    print("ğŸ“ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ output1.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    df = pd.read_csv("output1.csv", encoding="utf-8")
    df.to_excel("output1.xlsx", index=False)
    print("ğŸ“ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ output1.xlsx Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")


# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
crawl_batch()